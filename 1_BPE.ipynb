{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Use the \"TinyStories\" dataset, as we don't have the resources for a big and complicated dataset</h1>\n",
    "https://www.kaggle.com/datasets/thedevastator/tinystories-narrative-classification?resource=download\n",
    "\n",
    "<h3>This file produces the Byte Pair Encoding used; Our transformer does NOT produce individual characters, but rather fragments of sentences. Imagine monkeys on a type writer: If they're given buttons with words, you're much more likely to get a Shakespeare text eventually.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Load data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        #read text file\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        #merge all lines together:\n",
    "        text = \"\\n\".join(lines)\n",
    "        #replaced all \"\" with ` to make parsing the sequences easier (splitting them by \" then yields individual stories)\n",
    "        text = text.replace('\"\"', '`') \n",
    "        #split text into chunks according to \", i.e. one story each:\n",
    "        chunks = text.split('\"')\n",
    "        #clean: remove first chunk (csv name):\n",
    "        chunks = chunks[1:]\n",
    "        #remove every second chunk (empty stuff):\n",
    "        chunks = chunks[::2]\n",
    "        \n",
    "        self.data = chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "#we use the validation.csv file to produce the encoding;\n",
    "#this is a bit easier than using the (much larger) train file\n",
    "textfile = \"validation.csv\"\n",
    "\n",
    "dataset = TextDataset(textfile)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#give out something from the dataloader to test it:\n",
    "for batch in dataloader:\n",
    "    inputs = batch\n",
    "    print(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. For ease of processing, just gather all data in one string, then take a subset of it</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenise = \"\"\n",
    "for i in dataset.data:\n",
    "    text_to_tokenise += \" \" + i\n",
    "\n",
    "#get unique characters:\n",
    "chars = list(set(text_to_tokenise))\n",
    "\n",
    "#create a mapping from unique characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(chars)}\n",
    "\n",
    "#convert the text to a tensor\n",
    "text_as_indices = [char_to_index[char] for char in text_to_tokenise]\n",
    "text_subset_for_BPE = text_as_indices[:(1024*1024*4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helpers to transcribe characters to their token and vice-versa</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_chars_to_index(chars):\n",
    "    indices = []\n",
    "    for char in chars:\n",
    "        indices.append(char_to_index[char])\n",
    "    return indices\n",
    "\n",
    "def transcribe_indices_to_chars(indices):\n",
    "    return [index_to_char[indices[i]] for i in range(0, len(indices))]\n",
    "\n",
    "print(transcribe_indices_to_chars(transcribe_chars_to_index('abc')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Find BPE:</h1>\n",
    "-initially: tokens = characters; later: tokens = 1 to N many characters \"in one\"<br/>\n",
    "-find most frequent pair of tokens next to each other; usually, something like \"e \" is very frequent<br/>\n",
    "-replace most frequent pair with a new token everywhere<br/>\n",
    "-repeat until we reach a desired dictionary size<br/>\n",
    "<br/>\n",
    "(usually, you'd want to do this with some C++ code, python takes forever for longer texts/more tokens!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple BPE: use 2048 tokens, i.e. introduce 2048-EXISTING new tokens to the vocabulary\n",
    "DICT_LENGTH = 512 * 4\n",
    "\n",
    "#contains: [tokenIDA, tokenIDB] -> new index; we need these rules to later extract the tokens from the model\n",
    "rules = []\n",
    "if not os.path.exists(\"BPE\"): #skip and just load existing if we cancelled before\n",
    "    while len(index_to_char) < DICT_LENGTH:\n",
    "        #1. count pairs:\n",
    "        count = dict()\n",
    "        for i in range(0, len(text_subset_for_BPE)-1):\n",
    "            comb = (text_subset_for_BPE[i], text_subset_for_BPE[i+1])\n",
    "            if comb in count:\n",
    "                count[comb] += 1\n",
    "            else:\n",
    "                count[comb] = 1\n",
    "        #2. find most common pair:\n",
    "        max_key = max(count, key=count.get)\n",
    "        print(max_key)\n",
    "        print(\"BPE replaces |\"+index_to_char[max_key[0]]+index_to_char[max_key[1]]+\"| with a new token; it has \"+str(count[max_key])+\" occurences\")\n",
    "        #3. replace pair in text:\n",
    "        new_index = len(index_to_char)\n",
    "        index_to_char[new_index] = index_to_char[max_key[0]]+index_to_char[max_key[1]]\n",
    "        rules.append((max_key, new_index))\n",
    "        #replace all most common occurences with new the index:\n",
    "        i = 0\n",
    "        while i < len(text_subset_for_BPE)-1:\n",
    "            if text_subset_for_BPE[i] == max_key[0] and text_subset_for_BPE[i+1] == max_key[1]:\n",
    "                text_subset_for_BPE[i] = new_index\n",
    "                text_subset_for_BPE.pop(i+1)\n",
    "            i += 1\n",
    "        #(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save:\n",
    "#   -rules\n",
    "#   -index_to_char\n",
    "#   -char_to_index\n",
    "\n",
    "#check if directory \"BPE\" exists:\n",
    "if not os.path.exists(\"BPE\"):\n",
    "    os.makedirs(\"BPE\")\n",
    "    torch.save(rules, \"BPE/rules.pt\")\n",
    "    torch.save(index_to_char, \"BPE/index_to_char.pt\")\n",
    "    torch.save(char_to_index, \"BPE/char_to_index.pt\")\n",
    "else: #just load existing if we cancelled before\n",
    "    rules = torch.load(\"BPE/rules.pt\")\n",
    "    index_to_char = torch.load(\"BPE/index_to_char.pt\")\n",
    "    char_to_index = torch.load(\"BPE/char_to_index.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test with a demo from the dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Spot. Spot saw the shiny car and said, `Wow, Kitty, your car is so bright and clean!` Kitty smiled and replied, `Thank you, Spot. I polish it every day.`\"\n",
    "\n",
    "def apply_BPE(text, rules):\n",
    "    text_as_indices = transcribe_chars_to_index(text)\n",
    "    for rule in rules:\n",
    "        i = 0\n",
    "        while i < len(text_as_indices)-1:\n",
    "            if text_as_indices[i] == rule[0][0] and text_as_indices[i+1] == rule[0][1]:\n",
    "                text_as_indices[i] = rule[1]\n",
    "                text_as_indices.pop(i+1)\n",
    "            i += 1\n",
    "            \n",
    "    return text_as_indices\n",
    "\n",
    "def decode_BPE(tokens):\n",
    "    #return \"\".join(transcribe_indices_to_chars(tokens))\n",
    "    return transcribe_indices_to_chars(tokens) #output as list; that way, we can still see the individual tokens, which is useful for debugging\n",
    "\n",
    "print(\"LENGTH BEFORE: \", len(test))\n",
    "print(\"LENGTH AFTER: \", len(apply_BPE(test, rules)))\n",
    "print(\"DECODED: \",decode_BPE(apply_BPE(test, rules))) #do we get the same sentence out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Apply BPE to the training data and test data, then save as one big torch tensor</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Write out training data (split up into different files)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"data\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "textfile = \"train.csv\"\n",
    "dataset = TextDataset(textfile)\n",
    "#split train data into multiple individual files to handle things better:\n",
    "NO_CHUNKS = 128\n",
    "for chunk in range(0, NO_CHUNKS): #<-- if you cancel this, it will continue from the last chunk\n",
    "    #check if file already exists:\n",
    "    if os.path.exists(\"data/train_BPE_\"+str(chunk)+\".dat\"):\n",
    "        continue\n",
    "    fails = 0\n",
    "    new_data = []\n",
    "    start = time.time()\n",
    "    for i in range(chunk, len(dataset.data), NO_CHUNKS):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(\"\\tCHUNK \",chunk,\" - DONE WITH \", i/len(dataset.data)*100, \"%, time left: \", (time.time()-start)/i*(len(dataset.data)-i)/60, \" minutes\")\n",
    "        try:\n",
    "            dat = torch.tensor(apply_BPE(dataset.data[i], rules)).to(torch.int32)\n",
    "            new_data.append(dat)\n",
    "        except:\n",
    "            fails += 1\n",
    "            if fails % 100 == 0:\n",
    "                print(\"\\tSo far, \", fails, \"/\", i, \" failed\")\n",
    "            pass\n",
    "    torch.save(new_data, \"data/train_BPE_\"+str(chunk)+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Write out validation data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/validation_BPE.dat\"):\n",
    "    textfile = \"validation.csv\"\n",
    "    dataset = TextDataset(textfile)\n",
    "    start = time.time()\n",
    "    new_data = []\n",
    "    for i in range(0, len(dataset.data)):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(\"DONE WITH \", i/len(dataset.data)*100, \"%, time left: \", (time.time()-start)/i*(len(dataset.data)-i)/60, \" minutes\")\n",
    "        dat = torch.tensor(apply_BPE(dataset.data[i], rules)).to(torch.int32)\n",
    "        new_data.append(dat)\n",
    "\n",
    "    torch.save(new_data, \"data/validation_BPE.dat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
