{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Build a loader to get the data</h1>\n",
    "<h3>Note that we use the \"Schedule Free\" Adam from Facebook research: https://github.com/facebookresearch/schedule_free<br/>\n",
    "This relieves us from the annoying task of providing a learning rate schedule for our transformer (i.e. adapt the learning rate in some pattern throughout training);<br/>\n",
    "However, it still needs warmup.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "import schedulefree\n",
    "\n",
    "rules = torch.load(\"BPE/rules.pt\")\n",
    "index_to_char = torch.load(\"BPE/index_to_char.pt\")\n",
    "char_to_index = torch.load(\"BPE/char_to_index.pt\")\n",
    "\n",
    "PADDING_TOKEN = len(index_to_char) #padding token is the last token in the index_to_char dictionary; we use it to shorter pad sequences to the max length\n",
    "LARGEST_SEQUENCE_LENGTH = 250\n",
    "BATCH_SIZE = 16 #increase me - this is the number of sequences that are processed in parallel; should run on any decent GPU, I used 80 for my NVidia 4080\n",
    "TEXTFILES_TO_USE = 2 # if you want to use more than 2 textfiles, you need to run the first file to produce them from the original dataset\n",
    "\n",
    "#helper functions - the same as for our BPE\n",
    "def apply_BPE(text, rules):\n",
    "    text_as_indices = transcribe_chars_to_index(text)\n",
    "    for rule in rules:\n",
    "        i = 0\n",
    "        while i < len(text_as_indices)-1:\n",
    "            if text_as_indices[i] == rule[0][0] and text_as_indices[i+1] == rule[0][1]:\n",
    "                text_as_indices[i] = rule[1]\n",
    "                text_as_indices.pop(i+1)\n",
    "            i += 1\n",
    "            \n",
    "    return text_as_indices\n",
    "\n",
    "def transcribe_indices_to_chars(indices):\n",
    "    return [index_to_char[indices[i]] for i in range(0, len(indices))]\n",
    "\n",
    "def decode_BPE(tokens):\n",
    "    #EXPECTS input to be ONE item, not a batch!\n",
    "    #cut off every token >= PADDING_TOKEN!\n",
    "    tokens_ = tokens\n",
    "    tokens = []\n",
    "    for token in tokens_:\n",
    "        if token < PADDING_TOKEN:\n",
    "            #if token is a tensor, append item:\n",
    "            if isinstance(token, torch.Tensor):\n",
    "                tokens.append(token.item())\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "        else:\n",
    "            break\n",
    "    #return \"\".join(transcribe_indices_to_chars(tokens)) #show full text\n",
    "    return str(transcribe_indices_to_chars(tokens)) #show individual text fragments, in an array\n",
    "\n",
    "def transcribe_chars_to_index(chars):\n",
    "    indices = []\n",
    "    for char in chars:\n",
    "        indices.append(char_to_index[char])\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load from multiple files, pad to a fixed length / filter out longer ones</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.data = []\n",
    "        for file_path in file_paths:\n",
    "            loaded = torch.load(file_path)\n",
    "            self.data.extend(loaded)\n",
    "        print(\"Loaded \", len(self.data), \" samples from \", len(file_paths), \" files.\")\n",
    "        \n",
    "        if False: #print some statsitics; usually, throwing away samples with 250+ doesn't hurt much, but speeds up computation considerably\n",
    "            largest = 0 #find the largest sequence length\n",
    "            largest_sequence = None\n",
    "            lengths = []\n",
    "            for sample in self.data:\n",
    "                lengths.append(len(sample))\n",
    "                if len(sample) > largest:\n",
    "                    largest = len(sample)\n",
    "                    largest_sequence = sample\n",
    "            \n",
    "            print(\"Average sequence length: \", sum(lengths)/len(lengths))\n",
    "            print(\"Median sequence length: \", sorted(lengths)[len(lengths)//2])\n",
    "            print(\"90 percent length: \", sorted(lengths)[int(len(lengths)/10*8)])\n",
    "\n",
    "            print(\"Largest sequence length: \", largest)\n",
    "            print(\"Example of largest sequence: \", decode_BPE(largest_sequence))\n",
    "        \n",
    "        #throw away samples with LARGEST_SEQUENCE_LENGTH+ tokens\n",
    "        #   -> without this, attention computation is very slow (quadratic scaling) with little benefit (very few samples are actually that long)\n",
    "        self.data = [torch.cat((sample, torch.ones((LARGEST_SEQUENCE_LENGTH-len(sample),), dtype=torch.long) * PADDING_TOKEN)) for sample in self.data if len(sample) < LARGEST_SEQUENCE_LENGTH]\n",
    "        #stack:\n",
    "        self.data = torch.stack(self.data)\n",
    "        print(\"Stored \", len(self.data), \" tensors.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "#list all textfiles we want to load\n",
    "textfiles = []\n",
    "for i in range(0, TEXTFILES_TO_USE):\n",
    "    textfiles.append(\"data/train_BPE_\"+str(i)+\".dat\")\n",
    "\n",
    "train_dataset = EncodedDataset(textfiles)\n",
    "test_dataset = EncodedDataset([\"data/validation_BPE.dat\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Transformer helpers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(embed_dims, SEQUENCE_LENGTH):\n",
    "    enc = torch.arange(SEQUENCE_LENGTH).unsqueeze(1).float()  # Use arange instead of ones\n",
    "    denominator = torch.pow(10000, torch.arange(0, embed_dims, 2).float() / embed_dims)\n",
    "    \n",
    "    angle_rads = enc / denominator  # Element-wise division\n",
    "    sin_vals = torch.sin(angle_rads)\n",
    "    cos_vals = torch.cos(angle_rads)\n",
    "\n",
    "    # Interleave sin and cos values to match expected shape\n",
    "    pos_enc = torch.zeros(SEQUENCE_LENGTH, embed_dims)\n",
    "    pos_enc[:, 0::2] = sin_vals  # Even indices\n",
    "    pos_enc[:, 1::2] = cos_vals  # Odd indices\n",
    "\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Build actual Transformer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Rough rundown of what a transformer does:</h3>\n",
    "https://arxiv.org/abs/1706.03762 is the original idea; https://jalammar.github.io/illustrated-transformer/ explains it somewhat nicely:<br/>\n",
    "For attention, you compute pairwise scores between all tokens, then use these scores to<br/>\n",
    "mix your tokens together to new tokens. Exemplary, for \"A black cat sat on the wall\", the word \"black\" will \"attend\" to \"cat\", i.e. have a lot of attention on black;<br/>\n",
    "meaning the tokens will be mixed such that we have a hybrid token thingy that says \"black cat\" (very crude).<br/><br/>\n",
    "Transformers are just build out of stacked blocks (\"layers\"); each block consists of:<br/>\n",
    "-an attention layer that computes pairwise scores, then re-mixes tokens accordingly<br/>\n",
    "-normalisations & residuals<br/>\n",
    "-a fully connected network part that is applied to EACH token after attention; meaning this does the heavy lifting,<br/>\n",
    "while the attention is the only part where tokens get to know each other. The fully connected network is also the part where mixture of experts (MoE)<br/>\n",
    "usually happens, i.e. where we apply a different network according to some routing process (\"for math, we use net A, for french, net D, for german, net F, [...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code blocks here:</h3>\n",
    "<b>AutoregressiveDecoderTransformer</b> is a decoder-only transformer (i.e. predicts away token after token); it stacks multiple transformer decoder blocks,<br/>\n",
    "then applies some head that does classification (i.e. gives us a pseudo probability distributuon over tokens) that we sample from.<br/><br/>\n",
    "\n",
    "<b>TransformerDecoderBlock</b> is a transformer block: attention computation between all tokens, then apply the fully connected network to each token. Also contains normalisation and residuals<br/><br/>\n",
    "\n",
    "<b>FeedForward</b> is the fully connected network that processes each token after \"mixing\" it in the attention layer<br/><br/>\n",
    "\n",
    "<b>CausalSelfAttention</b> is there to speed up training: When we process a full sentence, we can just \"cover up\" some part of the attention matrix to get a subset that describes part of a sentence;<br/>\n",
    "if we have \"The| |black| |cat\", we can compute the whole 5-by-5 attention matrix (whole sentence) and just cover up pieces of it for e.g. \"The| |black\". This is what makes transformers so fast -<br/>\n",
    "they somewhat learn on each prefix in parallel.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "#set to 0.0 for no dropout\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    #feed forward with ReLU\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        #two linear layers with ReLU in between\n",
    "        #note how size expands and contracts:\n",
    "        #   imagine a puzzle you try to solve - you want the table you do it on\n",
    "        #   to be big enough to lay out all the pieces instead of just big enough \n",
    "        #   to hold the final result\n",
    "        self.lin_1 = torch.nn.Linear(dim, dim * 4)\n",
    "        self.lin_2 = torch.nn.Linear(dim * 4, dim)\n",
    "        #I'd suggest leaky ReLU, but ReLU is the standard used in TFs\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.lin_1(x))\n",
    "        x = self.lin_2(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    #>>Causal<< self-attention means that the model can only look at previous tokens:\n",
    "    #   this is important for autoregressive models, as we can then recycle a lot\n",
    "    #   of the computation for attention & train all prefixes at once\n",
    "    #   (e.g. for ABCDEFG, we train the next token after A, after AB, after ABC, ... in one go)\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ensure that we can split the dimension into num_heads\n",
    "        #each head will then have dim/num_heads dimensions, i.e.\n",
    "        #we divide the input into num_heads parts to \n",
    "        #   a) keep matrix sizes tame\n",
    "        #   b) ensure that different heads can focus on different tasks\n",
    "        #      (softmax focuses the attention largely on one part of the input,\n",
    "        #       then different heads can focus on different parts)\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        #scale factor for attention - as the dot product grows with the dimension, \n",
    "        #   we scale it down to prevent the softmax from getting too extreme / sharp\n",
    "        self.scale = 1.0 / (math.sqrt(self.head_dim))\n",
    "\n",
    "        #linear layer for query, key, value (=apply query, key, value matrix to input)\n",
    "        #here, we make life easy for us and apply one linear layer (=also multiplies the input by a matrix)\n",
    "        #   (instead of having separate matrices for each)\n",
    "        #   --> same number of parameters, but less code / all in one go\n",
    "        #   (don't forget the bias=False, as we don't want to add a bias here; we just want a matrix multiplication, essentially)\n",
    "        #   (linear layer is just W * x + b)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(dim, 3 * dim, bias=False)\n",
    "        \n",
    "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, T, C = x.shape\n",
    "        #produce key, query, value from input\n",
    "        qkv = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        #divide into num_heads parts (=split the dimension up)\n",
    "        q, k, v = map(lambda t: t.view(batch_size, T, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n",
    "\n",
    "        #compute scaled dot product attention:\n",
    "        #   dot product of query and key, then scale it down to prevent softmax from getting too extreme\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        #to prevent the model from looking at future tokens (for autoregressive training),\n",
    "        #   we mask the attention weights for tokens that are in the future;\n",
    "        #   this is done by setting the attention weights for future tokens to -inf\n",
    "        #   (as softmax(-inf) = 0, i.e. the model will ignore these tokens)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        out = (attn_weights @ v).transpose(1, 2).contiguous().view(batch_size, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    #transformer decoder block:\n",
    "    #   self attention, dropout\n",
    "    #   residual & layer norm\n",
    "    #   feed forward, dropout\n",
    "    #   residual & layer norm\n",
    "\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(dim, num_heads)\n",
    "        self.ffn = FeedForward(dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        #dropout just to prevent overfitting, i.e. memorising stuff:\n",
    "        #   we want the model to learn the structure of the data, not the data itself!\n",
    "        #   dropout is a simple way to prevent the model from memorising the data\n",
    "        #   by randomly setting some weights to zero;\n",
    "        #   i.e. the model can't rely on just memorising individual aspects,\n",
    "        #   but has to learn the structure of the data in a general and redundant (=robust) way\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        #self attention and dropout, then residual\n",
    "        x = x + self.dropout(self.attn(x, mask))\n",
    "        #layer norm\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        #feed forward and dropout, then residual\n",
    "        x = x + self.dropout(self.ffn(x))\n",
    "        #layer norm\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class AutoregressiveDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, dim, num_layers, num_heads, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        #embed tokens with something learnable\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        #embed positions with something computed\n",
    "        self.pos_embedding = torch.nn.Parameter(positional_encoding(dim, LARGEST_SEQUENCE_LENGTH+1)[None], requires_grad=False)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(dim, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(max_seq_len, max_seq_len)).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, tokens = x.shape\n",
    "        token_emb = self.token_embedding(x)\n",
    "        pos_emb = self.pos_embedding[:,:x.size()[1]]\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        mask = self.mask[:, :, :tokens, :tokens]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.decoder(x)\n",
    "\n",
    "    #maximum sampling - useful for debugging, but will always generate the same sequence (\"always pick the most likely token as next token\")\n",
    "    def generate_max(self, tokens, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(tokens)\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=False)[:, -1:]\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n",
    "    #sample just randomly according to probability - has a chance to pick some really messed up\n",
    "    def generate_mul(self, tokens, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(tokens)\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1)[:, -1], 1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n",
    "    #nucleus sampling: only sample from the top 90% of the probability distribution\n",
    "    #                  we do so by taking only the largest probabilities up to gaining 90%,\n",
    "    #                  then sampling from it\n",
    "    #                  in result, this will a) prevent unlikely stuff from being sampled\n",
    "    #                                   and b) still allows for some randomness, IMHO a bit nicer than top-k\n",
    "    def generate_nuc(self, tokens, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            #1. get logits\n",
    "            logits = self.forward(tokens)\n",
    "            #2. turn into probabilities\n",
    "            probs = F.softmax(logits, dim=-1)[:, -1]\n",
    "            #3. sort & cumsum to get the cumulative probability to cut off everything beyond 90%\n",
    "            sorted, indices = torch.sort(probs, descending=True)\n",
    "            cumulative = torch.cumsum(sorted, dim=-1)\n",
    "            #find the first index where the cumulative probability is larger than 0.9\n",
    "            cutoff = torch.argmax((cumulative > 0.9).long(), dim=-1)\n",
    "            #4. null out everything beyond 90%\n",
    "            for b in range(0, probs.size()[0]):\n",
    "                cutoff_index = cutoff[b] + 1\n",
    "                probs[b, indices[b, cutoff_index:]] = 0.0\n",
    "            #5. sample from the modified probabilities\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move model to GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoregressiveDecoderTransformer(PADDING_TOKEN + 1, LARGEST_SEQUENCE_LENGTH+1, dim=512, num_layers=8, num_heads=8, dropout=DROPOUT_RATE).to(DEVICE)\n",
    "print(\"Model has \", sum(p.numel() for p in model.parameters()), \" parameters.\")\n",
    "\n",
    "LR = 0.001 #works best for this model & dataset\n",
    "\n",
    "#linear warumup of learning rate; if you use AdamW instead of SFAdam, just apply that manually\n",
    "WARMUP_STEPS = 2500\n",
    "\n",
    "#how many (text) samples to generate after each epoch\n",
    "SAMPLES_TO_GENERATE = 4\n",
    "\n",
    "optimiser = schedulefree.AdamWScheduleFree(model.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=0.01, warmup_steps=WARMUP_STEPS)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "total_losses_train = []\n",
    "total_losses_test  = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "\n",
    "    #1. Train:\n",
    "    optimiser.train()\n",
    "    model.train()\n",
    "    \n",
    "    its = 0\n",
    "    start = time.time()\n",
    "    last = start\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        steps += 1\n",
    "        #pre-pad initial empty token (i.e. \"start of sequence\" token to also learn what the first \"real\" token should be)\n",
    "        batch = torch.cat((torch.ones(batch.size()[0], 1).long() * PADDING_TOKEN, batch), dim=1)\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        logits = model(batch)\n",
    "        \n",
    "        target = batch[:,1:] #shifted left by one; we want to predict the next token\n",
    "        output = logits[:,:-1] #remove the last prediction; we don't want to predict anything at the last token\n",
    "        \n",
    "        loss = loss_fn(output.reshape(-1, output.size(-1)), target.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimiser.step()\n",
    "        losses_train.append(loss.item())\n",
    "        its += 1\n",
    "        if time.time() - last > 30: \n",
    "            print(\"\\t\\tTime left for TRAIN epoch: \", (time.time()-start)/its*(len(train_dataloader)-its)/60, \" minutes; Currently, \",steps,\" steps in.\")\n",
    "            last = time.time()\n",
    "    \n",
    "    #2. Evaluate:\n",
    "    optimiser.eval()\n",
    "    model.eval()\n",
    "\n",
    "    #(important for SF AdamW: only store stuff when in eval mode!)\n",
    "    #save model:\n",
    "    if not os.path.exists(\"stored\"):\n",
    "        os.makedirs(\"stored\")\n",
    "    torch.save(model.state_dict(), \"stored/model_\"+str(epoch)+\".pt\")\n",
    "    #save optimiser:\n",
    "    torch.save(optimiser.state_dict(), \"stored/optimiser_\"+str(epoch)+\".pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        its = 0\n",
    "        start = time.time()\n",
    "        last = start\n",
    "        for batch in test_dataloader:\n",
    "            #pre-pad initial empty token\n",
    "            batch = torch.cat((torch.ones(batch.size()[0], 1).long() * PADDING_TOKEN, batch), dim=1)\n",
    "            batch = batch.to(DEVICE)\n",
    "            logits = model(batch)\n",
    "            \n",
    "            target = batch[:,1:] #shifted left by one; we want to predict the next token\n",
    "            output = logits[:,:-1] #remove the last prediction; we don't want to predict anything at the last token\n",
    "            \n",
    "            loss = loss_fn(output.reshape(-1, output.size(-1)), target.reshape(-1))\n",
    "            losses_test.append(loss.item())\n",
    "            its += 1\n",
    "            if time.time() - last > 30: \n",
    "                print(\"\\t\\tTime left for TEST epoch: \", (time.time()-start)/its*(len(train_dataloader)-its), \" seconds.\")\n",
    "                last = time.time()\n",
    "        \n",
    "        print(\"*** DONE WITH EPOCH \", epoch, \" - TRAIN LOSS: \", sum(losses_train)/len(losses_train), \" - TEST LOSS: \", sum(losses_test)/len(losses_test),\" ***\")\n",
    "        total_losses_train.append(sum(losses_train)/len(losses_train))\n",
    "        total_losses_test.append(sum(losses_test)/len(losses_test))\n",
    "\n",
    "        #save losses:\n",
    "        torch.save(total_losses_train, \"stored/total_losses_train_\"+str(epoch)+\".pt\")\n",
    "        torch.save(total_losses_test, \"stored/total_losses_test_\"+str(epoch)+\".pt\")\n",
    "        \n",
    "        plt.plot(total_losses_train, label=\"train\")\n",
    "        plt.plot(total_losses_test, label=\"test\")\n",
    "        plt.title(\"Losses at LR=\"+str(LR))\n",
    "        plt.legend()\n",
    "        #save plot & show:\n",
    "        plt.savefig(\"losses_\"+str(LR)+\"_\"+str(epoch)+\".png\")\n",
    "        plt.show()\n",
    "\n",
    "        #3. Inference / Generate:\n",
    "        #skip first character, that's just the empty token:\n",
    "        try:\n",
    "            sampled_max = model.generate_max((torch.ones(1, 1).long() * PADDING_TOKEN).to(DEVICE), 251)\n",
    "            sampled_mul = model.generate_mul((torch.ones(4, 1).long() * PADDING_TOKEN).to(DEVICE), 251)\n",
    "            sampled_nuc = model.generate_nuc((torch.ones(4, 1).long() * PADDING_TOKEN).to(DEVICE), 251)\n",
    "            print(\"\\tGENERATED SENTENCE  MAX: \")\n",
    "            print(\"\\t\\tMost likely story: \",decode_BPE(sampled_max[0, 1:]))\n",
    "            print(\"\\tGENERATED SENTENCE  MUL: \")\n",
    "            for i in range(0, SAMPLES_TO_GENERATE):\n",
    "                print(\"\\t\\tMultinomial sampling story \"+str(i)+\": \",decode_BPE(sampled_mul[i, 1:]))\n",
    "            print(\"\\tGENERATED SENTENCE  NUC: \")\n",
    "            for i in range(0, SAMPLES_TO_GENERATE):\n",
    "                print(\"\\t\\tNucleus (probably best) story \"+str(i)+\": \",decode_BPE(sampled_nuc[i, 1:]))\n",
    "        except:\n",
    "            print(\"FAILED TO GENERATE\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
