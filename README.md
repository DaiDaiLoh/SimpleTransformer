This repository contains a simple transformer implementation for language modeling in pytorch, from scratch, made to be both (reasonably) fast and still readable.<br/>
Note that this will <b>always</b> slower than the native pytorch version: It for example contains flash attention (tl;dr: optimisation to make attention computation much faster).<br/>
The inner workings, however, are the same.<br/>
<br/>
<h1>Contents:</h1>
-a preprocessing file that loads a dataset & computes a proper byte pair encoding (BPE), compressing individual characters into larger text fragments to ease training<br/>
-the actual transformer
<h1>How to run:</h1>
You can simply download & execute the second file; This will only use 1/128th of the full dataset though; If you want the full dataset, download the "TinyStories" dataset and follow the instructions in the first file.
<h3>Dataset:</h3>
We use the "TinyStories" dataset, about 2m childrens stories generated by OpenAI's GPT. This is a simplistic test for LLMs, as everything more complicated will easily exceed the compute at home.<br/>
https://arxiv.org/abs/2305.07759
